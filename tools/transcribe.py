import sys
import os
import argparse
from pathlib import Path
from .utils import check_environment

def transcribe_openai(audio_path, model_name="whisper-1", language="zh"):
    """ä½¿ç”¨ OpenAI API è½¬å½•"""
    try:
        from openai import OpenAI
    except ImportError:
        return None

    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    
    if not api_key:
        print("âŒ æœªé…ç½® OPENAI_API_KEY")
        return None
        
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    print(f"â˜ï¸ ä½¿ç”¨ API ({model_name}) è½¬å½•ä¸­...")
    try:
        with open(audio_path, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model=model_name,
                file=f,
                response_format="verbose_json",
                language=language if language != "auto" else None
            )
        return transcript.text
    except Exception as e:
        print(f"âŒ API é”™è¯¯: {e}")
        return None

def transcribe_local(audio_path, model_name, language):
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è½¬å½• (Whisper/Qwen)"""
    device = "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") != "-1" else "cpu"
    # ç®€å•æ£€æµ‹ cuda
    try:
        import torch
        if torch.cuda.is_available(): device = "cuda"
    except: pass
    
    print(f"ğŸ–¥ï¸ ä½¿ç”¨æœ¬åœ°æ¨¡å‹ ({model_name}) è®¾å¤‡: {device}")

    if "qwen" in model_name.lower():
        # Transformers pipeline for Qwen
        try:
            from transformers import pipeline
            import torch
            pipe = pipeline(
                "automatic-speech-recognition",
                model=model_name,
                device=device,
                trust_remote_code=True
            )
            result = pipe(str(audio_path), chunk_length_s=30, batch_size=8)
            return result["text"]
        except Exception as e:
            print(f"âŒ Qwen åŠ è½½å¤±è´¥: {e}")
            return None
    else:
        # Faster Whisper
        try:
            from faster_whisper import WhisperModel
            model = WhisperModel(model_name, device=device, compute_type="float16" if device=="cuda" else "int8")
            segments, info = model.transcribe(str(audio_path), language=language, beam_size=5)
            
            text = []
            print(f"æ£€æµ‹è¯­è¨€: {info.language} (æ¦‚ç‡: {info.language_probability:.2f})")
            for segment in segments:
                print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
                text.append(segment.text)
            return "\n".join(text)
        except Exception as e:
            print(f"âŒ Whisper å‡ºé”™: {e}")
            return None

if __name__ == "__main__":
    check_environment("transcribe")
    
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except: pass

    parser = argparse.ArgumentParser(description="è¯­éŸ³è½¬å†™å·¥å…·")
    parser.add_argument("file", help="éŸ³é¢‘/è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", "-m", default="base", help="æ¨¡å‹åç§° (base/large-v3/openai/Qwen...)")
    parser.add_argument("--language", "-l", default="zh", help="è¯­è¨€ä»£ç  (ä»…Whisperæœ‰æ•ˆ)")
    
    args = parser.parse_args()
    
    path = Path(args.file)
    if not path.exists():
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        sys.exit(1)
        
    if args.model.lower() in ["openai", "whisper-1"]:
        text = transcribe_openai(path, "whisper-1", args.language)
    else:
        text = transcribe_local(path, args.model, args.language)
        
    if text:
        out_path = path.with_suffix(".txt")
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"\nâœ… è½¬å½•å®Œæˆ! å·²ä¿å­˜è‡³: {out_path}")
