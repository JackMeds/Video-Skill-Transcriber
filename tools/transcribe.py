import sys
import os
import argparse
import time
import json
import warnings
from pathlib import Path
from .utils import check_environment

# Suppress warnings
warnings.filterwarnings("ignore")

try:
    import google.generativeai as genai
except ImportError:
    genai = None

def transcribe_gemini(file_path):
    """Use Google Gemini to transcribe/analyze video directly."""
    if not genai:
        print("âŒ google-generativeai not installed. Run: pip install google-generativeai")
        return None
    
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEY not found in environment variables.")
        return None
        
    genai.configure(api_key=api_key)
    
    print(f"   ğŸ“¤ Uploading {file_path.name} to Gemini (Multimodal)...")
    # Upload file
    try:
        video_file = genai.upload_file(path=file_path)
    except Exception as e:
        print(f"âŒ Upload failed: {e}")
        return None
    
    # Wait for processing
    print("   â³ Processing video...")
    while video_file.state.name == "PROCESSING":
        print('.', end='', flush=True)
        time.sleep(2)
        video_file = genai.get_file(video_file.name)
        
    if video_file.state.name == "FAILED":
        print("\nâŒ Google Server failed to process file.")
        return None
    
    print("\n   ğŸ¤– Analyzing with Gemini 1.5 Flash...")
    # Using 1.5 Flash by default as it's fast and supports video
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    
    prompt = "Please provide a verbatim transcription of this video/audio. If there are visual elements, describe them briefly in brackets."
    
    try:
        response = model.generate_content([video_file, prompt])
        return response.text
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        return None

def transcribe_openai(audio_path, model_name="whisper-1", language="zh"):
    """ä½¿ç”¨ OpenAI API è½¬å½•"""
    try:
        from openai import OpenAI
    except ImportError:
        print("âŒ openai package not installed")
        return None

    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    
    if not api_key:
        print("âŒ æœªé…ç½® OPENAI_API_KEY")
        return None
        
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    print(f"â˜ï¸ ä½¿ç”¨ API ({model_name}) è½¬å½•ä¸­...")
    try:
        with open(audio_path, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model=model_name,
                file=f,
                response_format="verbose_json",
                language=language if language != "auto" else None
            )
        return transcript.text
    except Exception as e:
        print(f"âŒ API é”™è¯¯: {e}")
        return None

def transcribe_local(audio_path, model_name, language):
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è½¬å½• (Whisper/Qwen)"""
    device = "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") != "-1" else "cpu"
    # ç®€å•æ£€æµ‹ cuda
    try:
        import torch
        if torch.cuda.is_available(): device = "cuda"
    except: pass
    
    print(f"ğŸ–¥ï¸ ä½¿ç”¨æœ¬åœ°æ¨¡å‹ ({model_name}) è®¾å¤‡: {device}")

    if "qwen" in model_name.lower():
        # Transformers pipeline for Qwen
        try:
            from transformers import pipeline
            pipe = pipeline(
                "automatic-speech-recognition",
                model=model_name,
                device=device,
                trust_remote_code=True
            )
            result = pipe(str(audio_path), chunk_length_s=30, batch_size=8)
            return result["text"]
        except Exception as e:
            print(f"âŒ Qwen åŠ è½½å¤±è´¥: {e}")
            return None
    else:
        # Faster Whisper
        try:
            from faster_whisper import WhisperModel
            model = WhisperModel(model_name, device=device, compute_type="float16" if device=="cuda" else "int8")
            segments, info = model.transcribe(str(audio_path), language=language, beam_size=5)
            
            text = []
            print(f"æ£€æµ‹è¯­è¨€: {info.language} (æ¦‚ç‡: {info.language_probability:.2f})")
            for segment in segments:
                print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
                text.append(segment.text)
            return "\n".join(text)
        except Exception as e:
            print(f"âŒ Whisper å‡ºé”™: {e}")
            return None

if __name__ == "__main__":
    check_environment("transcribe")
    
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except: pass

    parser = argparse.ArgumentParser(description="è¯­éŸ³/è§†é¢‘è½¬å†™å·¥å…·")
    parser.add_argument("file", help="éŸ³é¢‘/è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", "-m", default="base", help="æ¨¡å‹åç§° (base/large-v3/openai/gemini/Qwen...)")
    parser.add_argument("--language", "-l", default="zh", help="è¯­è¨€ä»£ç  (ä»…Whisperæœ‰æ•ˆ)")
    
    args = parser.parse_args()
    
    path = Path(args.file)
    if not path.exists():
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        sys.exit(1)
        
    model_lower = args.model.lower()
    
    if "gemini" in model_lower:
        text = transcribe_gemini(path)
    elif model_lower in ["openai", "whisper-1"]:
        text = transcribe_openai(path, "whisper-1", args.language)
    else:
        text = transcribe_local(path, args.model, args.language)
        
    if text:
        out_path = path.with_suffix(".txt")
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"\nâœ… è½¬å½•å®Œæˆ! å·²ä¿å­˜è‡³: {out_path}")
